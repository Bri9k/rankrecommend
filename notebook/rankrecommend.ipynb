{
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "scrape.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "",
  "signature": "sha256:9887204db641c4e0de61c85a176d37edbda18b2f788a804c0d54ee9c04b906f0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Github Follow Recommendation\n",
      "\n",
      "> Given a user, build an network surrounding the user and generate follow recommendation for him"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Graph Construction:\n",
      "\n",
      "* Given a github username, the following code uses a modified breadth-first search to build a graph of users centred around the root\n",
      "\n",
      "* The code uses *requests* to scrape github based on the followers list, and *BeautifulSoup* to extract information from the html pages. (I used 'inspect element' in firefox to determine the tags that are to be isolated)\n",
      "\n",
      "* The basic logic is this- every iteration, the code 'visists' and unvisited user, and adds his followers and followees to the 'unvisited' list. It also adds an edge from each follower to user and from user to each followee. This, it does till a specified number of nodes are added to the graph.\n",
      "\n",
      "* The graph is then written to a file (better to have reusable data files than re-running the code everytime)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import requests as rq\n",
      "import networkx as nx\n",
      "\n",
      "root = \"Bri9k\"\n",
      "filename = \"huge_graph.txt\"\n",
      "\n",
      "unvisited = [root]\n",
      "visited = set()\n",
      "\n",
      "gitgraph = nx.DiGraph()\n",
      "\n",
      "while len(gitgraph) < 1000:\n",
      "    newnode = unvisited[0]\n",
      "\n",
      "    \n",
      "    url = \"https://github.com/\" + newnode + \"/followers\"\n",
      "    print(\"Visiting \" + url)\n",
      "    resource = rq.get(url)   \n",
      "    follower_raw = resource.text\n",
      "    follower_soup = BeautifulSoup(follower_raw)\n",
      "    preprocess_list = follower_soup.find_all(\"a\", class_ = \"d-inline-block\")\n",
      "    try:\n",
      "        names = [user['href'][1:] for user in preprocess_list]\n",
      "        # print(names)\n",
      "        for hrudaya in names[2:]:\n",
      "            gitgraph.add_edge(hrudaya, newnode)\n",
      "            if hrudaya not in visited:\n",
      "                unvisited.append(hrudaya)\n",
      "    except:\n",
      "        print(\"Unable to find folowers for \" + newnode)\n",
      "    \n",
      "    \n",
      "    url2 = \"https://github.com/\" + newnode + \"/following\"\n",
      "    print(\"Visiting \" + url2)\n",
      "    resource = rq.get(url2)\n",
      "    follower_raw = resource.text\n",
      "    follower_soup = BeautifulSoup(follower_raw)\n",
      "    preprocess_list = follower_soup.find_all(\"a\", class_ = \"d-inline-block\")\n",
      "    try:\n",
      "        names = [user['href'][1:] for user in preprocess_list]\n",
      "        # print(names)\n",
      "        for amala in names[2:]:\n",
      "            gitgraph.add_edge(newnode, amala)\n",
      "            if amala not in visited:\n",
      "                unvisited.append(amala)\n",
      "    except:\n",
      "        print(\"Unable to find folowees for \" + newnode)\n",
      "\n",
      "    visited.add(unvisited.pop(0))\n",
      "\n",
      "    \n",
      "github_graph = list(gitgraph.edges())\n",
      "\n",
      "links = open(filename, \"w\")\n",
      "\n",
      "for link in github_graph:\n",
      "    links.write(link[0] + \"\\t\" + link[1] + \"\\n\")\n",
      "\n",
      "links.close()"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "colab_type": "code",
      "id": "MOSa4zIv40Ow",
      "outputId": "3eca9f7b-8573-44cf-9258-4c0bed17760d"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/",
       "height": 35
      },
      "colab_type": "code",
      "id": "P_vVsIxu50OE",
      "outputId": "100c5728-0b61-4463-a7f0-0b3a4c93d4ff"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Basic Anylisis:\n",
      "\n",
      "* Builds a graph from the scraped file, runs basic pagerank on it (to determine influencial users without root), draws it, and dislplays the list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "gitgraph = nx.DiGraph()\n",
      "\n",
      "graphfile = open(\"huge_graph.txt\", \"r\")\n",
      "\n",
      "reader = csv.reader(graphfile, delimiter = \"\\t\")\n",
      "\n",
      "for line in reader:\n",
      "    gitgraph.add_edge(line[0], line[1])\n",
      "\n",
      "nx.info(gitgraph)\n",
      "\n",
      "userrank_dict = nx.pagerank(gitgraph)\n",
      "\n",
      "sorte = sorted(userrank_dict, key = lambda k: userrank_dict[k], reverse = True)\n",
      "\n",
      "for user in sorte:\n",
      "  print(user, userrank_dict[user])\n",
      "\n",
      "op = open(\"ranks.txt\", \"w\")\n",
      "for user in sorte:\n",
      "    op.write(user + \"    \" + str(userrank_dict[user]) + \"\\n\")\n",
      "\n",
      "op.close()\n",
      "\n",
      "nx.draw_spring(gitgraph, with_labels = False, node_size = 2, width = 0.1, arrows = False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/",
       "height": 18436
      },
      "colab_type": "code",
      "id": "7qIb4m9S6HaM",
      "outputId": "c51ad988-0b43-43f7-8a68-65ee85576195"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Recommendation\n",
      "\n",
      "* Linkpredict is a module I have written, which provides 3 recommender functions- adamic-adar, rooted pagerank and propflow. Rooted pagerank is the best one for directed graphs. In this case, the data is a digraph of github users\n",
      "\n",
      "* The function defined finds the rooted pagerank of all users in the graph with the user as the root.\n",
      "r\n",
      "* Rooted pagerank works thus: it finds the **eigenvectors** of the random walk transformation for the graph. The modification which allows recommendation is thus:\n",
      "\n",
      "> With probability 'k' walk randomly along one of the edges with node as source (equivalent to multplying the state vector with the matrix with diagonal elements being 1/(no_of_outgoing_edges) if there is an edge to the node at that row.) With probability (1 - k) *teleport* back to the user's node (equivalent to adding (1 - k) * [state vector with 1 at position of the user, and all others 0s]\n",
      "\n",
      "* Repeatedly applying this transofmation will give a steady state vector which is the final pagerank. \n",
      "\n",
      "* The users having the highest pagerank are the ones recommended.\n",
      "\n",
      "* Now of the users recommended, remove the user, people he is already following, and sort the remaining according to score.\n",
      "\n",
      "* Of these, chose the top 'n' ones- these the recommendations\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from linkpredict import rooted_pagerank\n",
      "\n",
      "def get_recommendations(graph, node, n):\n",
      "  recommendation_list = rooted_pagerank(graph, node, 0.5, 1e-6)\n",
      "  get_val = lambda key: recommendation_list[key]\n",
      "  top_recommendations = sorted(recommendation_list.keys(), key = get_val, reverse = True)\n",
      "  recommends = []\n",
      "  found = 0\n",
      "  i = 0\n",
      "  while len(recommends) < n:\n",
      "    if not graph.has_edge(node, top_recommendations[i]) and node != top_recommendations[i]:\n",
      "      recommends.append((top_recommendations[i], recommendation_list[top_recommendations[i]]))\n",
      "    i += 1\n",
      "                     \n",
      "  # print(recommends)\n",
      "      \n",
      "                     \n",
      "  return recommends\n",
      "    \n",
      "    \n",
      "  "
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/",
       "height": 386
      },
      "colab_type": "code",
      "id": "aFqbjjV8CmNc",
      "outputId": "cc338af6-fd6c-4bae-dc9a-1757995ab2b3"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recommends = get_recommendations(gitgraph, root, 10)\n",
      "\n",
      "for r in recommends:\n",
      "  print(r[0], r[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# References:\n",
      "\n",
      "1. https://en.wikipedia.org/wiki/PageRank\n",
      "\n",
      "2. \"The Link Prediction problem in social networks\"\n",
      "\n",
      "3. \"New perspectives and networks in link prediction\"\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}